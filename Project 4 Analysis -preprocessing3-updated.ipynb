{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ced39d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diabetes_011</th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoke</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysicalActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>...</th>\n",
       "      <th>AlcoholConsump</th>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245489</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245490</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245491</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245492</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245493</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>245494 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Diabetes_011  HighBP  HighChol  CholCheck  BMI  Smoke  Stroke  \\\n",
       "0                  0       0         1          0   40      1       0   \n",
       "1                  0       0         0          0   25      1       0   \n",
       "2                  0       1         1          1   28      0       0   \n",
       "3                  0       1         0          1   27      0       0   \n",
       "4                  0       1         1          1   24      0       0   \n",
       "...              ...     ...       ...        ...  ...    ...     ...   \n",
       "245489             0       0         0          1   20      0       0   \n",
       "245490             0       0         1          1   25      0       0   \n",
       "245491             1       1         0          1   22      0       0   \n",
       "245492             1       1         0          1   36      1       0   \n",
       "245493             0       1         1          1   33      1       0   \n",
       "\n",
       "        HeartDiseaseorAttack  PhysicalActivity  Fruits  ...  AlcoholConsump  \\\n",
       "0                          0                 0       0  ...               0   \n",
       "1                          0                 1       0  ...               0   \n",
       "2                          0                 0       1  ...               0   \n",
       "3                          0                 1       1  ...               0   \n",
       "4                          0                 1       1  ...               0   \n",
       "...                      ...               ...     ...  ...             ...   \n",
       "245489                     0                 1       1  ...               0   \n",
       "245490                     0                 1       1  ...               0   \n",
       "245491                     0                 1       1  ...               0   \n",
       "245492                     0                 0       1  ...               0   \n",
       "245493                     0                 1       1  ...               0   \n",
       "\n",
       "        AnyHealthcare  NoDocbcCost  GenHlth  MentHlth  PhysHlth  DiffWalk  \\\n",
       "0                   1            0        5        18        15         1   \n",
       "1                   0            1        3         0         0         0   \n",
       "2                   1            1        5        30        30         1   \n",
       "3                   1            0        2         0         0         0   \n",
       "4                   1            0        2         3         0         0   \n",
       "...               ...          ...      ...       ...       ...       ...   \n",
       "245489              0            1        4         0         4         1   \n",
       "245490              1            0        3         0         0         0   \n",
       "245491              1            0        3         0         0         0   \n",
       "245492              1            0        2         0         0         0   \n",
       "245493              1            1        2         3         0         0   \n",
       "\n",
       "        Sex  Age  Education  \n",
       "0         0    9          4  \n",
       "1         0    7          6  \n",
       "2         0    9          4  \n",
       "3         0   11          3  \n",
       "4         0   11          5  \n",
       "...     ...  ...        ...  \n",
       "245489    0    1          4  \n",
       "245490    0    9          4  \n",
       "245491    0    8          3  \n",
       "245492    1    4          4  \n",
       "245493    1    6          3  \n",
       "\n",
       "[245494 rows x 21 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Import our input dataset\n",
    "df = pd.read_csv('Resource/Diabetes_Prediction_Updated.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02891ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diabetes_011            0\n",
      "HighBP                  0\n",
      "HighChol                0\n",
      "CholCheck               0\n",
      "BMI                     0\n",
      "Smoke                   0\n",
      "Stroke                  0\n",
      "HeartDiseaseorAttack    0\n",
      "PhysicalActivity        0\n",
      "Fruits                  0\n",
      "Veggies                 0\n",
      "AlcoholConsump          0\n",
      "AnyHealthcare           0\n",
      "NoDocbcCost             0\n",
      "GenHlth                 0\n",
      "MentHlth                0\n",
      "PhysHlth                0\n",
      "DiffWalk                0\n",
      "Sex                     0\n",
      "Age                     0\n",
      "Education               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#count the number od missing values in each column\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59504493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = df[\"Diabetes_012\"].values\n",
    "#X = df.drop(columns=[\"Diabetes_012\"]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa5a5232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b962a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df_scaled = df.drop(columns=[\"Diabetes_012\"])\n",
    "#df_scaled.columns = df_scaled.columns.astype(str)\n",
    "#diabete_scaled = StandardScaler().fit_transform(df_scaled)\n",
    "#df_scaled = pd.DataFrame(diabete_scaled)\n",
    "\n",
    "y = df[\"Diabetes_011\"].values\n",
    "X = df.drop(columns=[\"Diabetes_011\"]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6dfecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X and y are your feature and target matrices\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccecc388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(184120, 20)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e3541e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tina\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1ms/step - accuracy: 0.8589 - loss: 0.3458\n",
      "Epoch 2/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8647 - loss: 0.3175\n",
      "Epoch 3/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8649 - loss: 0.3166\n",
      "Epoch 4/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8657 - loss: 0.3130\n",
      "Epoch 5/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8660 - loss: 0.3131\n",
      "Epoch 6/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8662 - loss: 0.3132\n",
      "Epoch 7/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8657 - loss: 0.3136\n",
      "Epoch 8/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8650 - loss: 0.3131\n",
      "Epoch 9/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8647 - loss: 0.3149\n",
      "Epoch 10/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8667 - loss: 0.3115\n",
      "Epoch 11/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8650 - loss: 0.3149\n",
      "Epoch 12/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8667 - loss: 0.3128\n",
      "Epoch 13/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8655 - loss: 0.3135\n",
      "Epoch 14/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8665 - loss: 0.3112\n",
      "Epoch 15/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8675 - loss: 0.3108\n",
      "Epoch 16/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8661 - loss: 0.3126\n",
      "Epoch 17/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8654 - loss: 0.3142\n",
      "Epoch 18/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8666 - loss: 0.3124\n",
      "Epoch 19/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8670 - loss: 0.3119\n",
      "Epoch 20/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8660 - loss: 0.3137\n",
      "Epoch 21/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8668 - loss: 0.3116\n",
      "Epoch 22/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8665 - loss: 0.3111\n",
      "Epoch 23/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8670 - loss: 0.3106\n",
      "Epoch 24/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8664 - loss: 0.3126\n",
      "Epoch 25/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8670 - loss: 0.3117\n",
      "Epoch 26/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8683 - loss: 0.3103\n",
      "Epoch 27/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8651 - loss: 0.3122\n",
      "Epoch 28/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8670 - loss: 0.3105\n",
      "Epoch 29/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8681 - loss: 0.3101\n",
      "Epoch 30/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8675 - loss: 0.3097\n",
      "Epoch 31/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8664 - loss: 0.3123\n",
      "Epoch 32/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8670 - loss: 0.3110\n",
      "Epoch 33/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8676 - loss: 0.3099\n",
      "Epoch 34/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8668 - loss: 0.3107\n",
      "Epoch 35/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8667 - loss: 0.3106\n",
      "Epoch 36/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8669 - loss: 0.3113\n",
      "Epoch 37/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8656 - loss: 0.3120\n",
      "Epoch 38/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8674 - loss: 0.3100\n",
      "Epoch 39/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8687 - loss: 0.3086\n",
      "Epoch 40/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8673 - loss: 0.3092\n",
      "Epoch 41/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8670 - loss: 0.3107\n",
      "Epoch 42/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8670 - loss: 0.3111\n",
      "Epoch 43/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8663 - loss: 0.3096\n",
      "Epoch 44/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8687 - loss: 0.3089\n",
      "Epoch 45/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8683 - loss: 0.3089\n",
      "Epoch 46/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8679 - loss: 0.3092\n",
      "Epoch 47/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8673 - loss: 0.3101\n",
      "Epoch 48/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8660 - loss: 0.3115\n",
      "Epoch 49/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8686 - loss: 0.3087\n",
      "Epoch 50/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8688 - loss: 0.3070\n",
      "Epoch 51/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - accuracy: 0.8664 - loss: 0.3111\n",
      "Epoch 52/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8678 - loss: 0.3095\n",
      "Epoch 53/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8674 - loss: 0.3091\n",
      "Epoch 54/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - accuracy: 0.8667 - loss: 0.3102\n",
      "Epoch 55/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8681 - loss: 0.3100\n",
      "Epoch 56/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8666 - loss: 0.3108\n",
      "Epoch 57/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8679 - loss: 0.3087\n",
      "Epoch 58/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8678 - loss: 0.3078\n",
      "Epoch 59/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8688 - loss: 0.3081\n",
      "Epoch 60/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8672 - loss: 0.3097\n",
      "Epoch 61/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8692 - loss: 0.3070\n",
      "Epoch 62/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8675 - loss: 0.3095\n",
      "Epoch 63/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8672 - loss: 0.3094\n",
      "Epoch 64/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8667 - loss: 0.3097\n",
      "Epoch 65/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8667 - loss: 0.3124\n",
      "Epoch 66/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8674 - loss: 0.3101\n",
      "Epoch 67/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8676 - loss: 0.3095\n",
      "Epoch 68/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8678 - loss: 0.3088\n",
      "Epoch 69/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8675 - loss: 0.3105\n",
      "Epoch 70/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8678 - loss: 0.3091\n",
      "Epoch 71/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8668 - loss: 0.3085\n",
      "Epoch 72/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8682 - loss: 0.3077\n",
      "Epoch 73/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8683 - loss: 0.3095\n",
      "Epoch 74/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8685 - loss: 0.3081\n",
      "Epoch 75/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8681 - loss: 0.3083\n",
      "Epoch 76/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8681 - loss: 0.3085\n",
      "Epoch 77/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8673 - loss: 0.3089\n",
      "Epoch 78/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8671 - loss: 0.3097\n",
      "Epoch 79/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8685 - loss: 0.3096\n",
      "Epoch 80/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8686 - loss: 0.3098\n",
      "Epoch 81/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8670 - loss: 0.3100\n",
      "Epoch 82/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8678 - loss: 0.3098\n",
      "Epoch 83/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8678 - loss: 0.3093\n",
      "Epoch 84/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8675 - loss: 0.3105\n",
      "Epoch 85/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8669 - loss: 0.3099\n",
      "Epoch 86/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8675 - loss: 0.3092\n",
      "Epoch 87/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8674 - loss: 0.3099\n",
      "Epoch 88/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8672 - loss: 0.3091\n",
      "Epoch 89/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8687 - loss: 0.3083\n",
      "Epoch 90/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8678 - loss: 0.3095\n",
      "Epoch 91/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8661 - loss: 0.3102\n",
      "Epoch 92/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8680 - loss: 0.3083\n",
      "Epoch 93/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8676 - loss: 0.3092\n",
      "Epoch 94/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8678 - loss: 0.3087\n",
      "Epoch 95/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8671 - loss: 0.3105\n",
      "Epoch 96/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8674 - loss: 0.3109\n",
      "Epoch 97/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8673 - loss: 0.3080\n",
      "Epoch 98/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8681 - loss: 0.3093\n",
      "Epoch 99/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8669 - loss: 0.3091\n",
      "Epoch 100/100\n",
      "\u001b[1m5754/5754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8675 - loss: 0.3096\n",
      "1918/1918 - 2s - 1ms/step - accuracy: 0.8667 - loss: 0.3129\n",
      "Loss: 0.31289494037628174, Accuracy: 0.8666536211967468\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of classes\n",
    "num_classes = len(set(y_train))\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "encoder = OneHotEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_test_encoded = encoder.transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "# Define the deep learning model \n",
    "nn_model = tf.keras.models.Sequential()\n",
    "nn_model.add(tf.keras.layers.Dense(units=21, activation=\"relu\", input_dim=X_train.shape[1]))\n",
    "nn_model.add(tf.keras.layers.Dense(units=20, activation=\"relu\"))\n",
    "nn_model.add(tf.keras.layers.Dense(units=num_classes, activation=\"softmax\"))\n",
    "\n",
    "# Compile the Sequential model with \"categorical_crossentropy\" loss\n",
    "nn_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "fit_model = nn_model.fit(X_train, y_train_encoded, epochs=100, verbose=1)\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn_model.evaluate(X_test, y_test_encoded, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a7300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=10)  # Choose the number of components as desired\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, random_state=42, stratify=y)\n",
    "\n",
    "# Calculate the number of classes\n",
    "num_classes = len(set(y_train))\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "encoder = OneHotEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_test_encoded = encoder.transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "# Define the deep learning model \n",
    "nn_model = tf.keras.models.Sequential()\n",
    "nn_model.add(tf.keras.layers.Dense(units=21, activation=\"relu\", input_dim=X_train.shape[1]))\n",
    "nn_model.add(tf.keras.layers.Dense(units=20, activation=\"relu\"))\n",
    "nn_model.add(tf.keras.layers.Dense(units=num_classes, activation=\"softmax\"))\n",
    "\n",
    "# Compile the Sequential model with \"categorical_crossentropy\" loss\n",
    "nn_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "fit_model = nn_model.fit(X_train, y_train_encoded, epochs=100, verbose=1)\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn_model.evaluate(X_test, y_test_encoded, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1c012b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Columns:\n",
      "PC1: 18.23%\n",
      "PC2: 11.83%\n",
      "PC3: 10.78%\n",
      "PC4: 9.73%\n",
      "PC5: 7.97%\n",
      "PC6: 6.99%\n",
      "PC7: 5.72%\n",
      "PC8: 5.40%\n",
      "PC9: 3.81%\n",
      "PC10: 3.16%\n"
     ]
    }
   ],
   "source": [
    "print(\"PCA Columns:\")\n",
    "for i in range(X_pca.shape[1]):\n",
    "    print(f\"PC{i+1}: {pca.explained_variance_ratio_[i]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7004cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8363855722492887"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26a8c048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Accuracy for Health Wellness columns*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "883f6523",
   "metadata": {},
   "outputs": [],
   "source": [
    "Health_wellness = df[[\"HighBP\",\"HighChol\",\"CholCheck\",\"BMI\",\"DiffWalk\"]]\n",
    "#Health_wellness.columns = Health_wellness.columns.astype(str)\n",
    "#Health_scaled = StandardScaler().fit_transform(Health_wellness)\n",
    "#Health_scaled = pd.DataFrame(Health_scaled)\n",
    "\n",
    "y = df[\"Diabetes_012\"].values\n",
    "X = Health_wellness.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b3fa51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diabetes_012</th>\n",
       "      <th>Smoke</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>Veggies</th>\n",
       "      <th>AlcoholConsump</th>\n",
       "      <th>PhysicalActivity</th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiffWalk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Diabetes_012</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.064170</td>\n",
       "      <td>-0.041679</td>\n",
       "      <td>-0.058731</td>\n",
       "      <td>-0.057718</td>\n",
       "      <td>-0.120440</td>\n",
       "      <td>0.271323</td>\n",
       "      <td>0.209294</td>\n",
       "      <td>0.067293</td>\n",
       "      <td>0.225294</td>\n",
       "      <td>0.223192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Smoke</th>\n",
       "      <td>0.064170</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.079937</td>\n",
       "      <td>-0.031694</td>\n",
       "      <td>0.101261</td>\n",
       "      <td>-0.091409</td>\n",
       "      <td>0.098372</td>\n",
       "      <td>0.092312</td>\n",
       "      <td>-0.009377</td>\n",
       "      <td>0.013567</td>\n",
       "      <td>0.124813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fruits</th>\n",
       "      <td>-0.041679</td>\n",
       "      <td>-0.079937</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.255008</td>\n",
       "      <td>-0.036000</td>\n",
       "      <td>0.141601</td>\n",
       "      <td>-0.040013</td>\n",
       "      <td>-0.040837</td>\n",
       "      <td>0.024418</td>\n",
       "      <td>-0.088325</td>\n",
       "      <td>-0.048510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Veggies</th>\n",
       "      <td>-0.058731</td>\n",
       "      <td>-0.031694</td>\n",
       "      <td>0.255008</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.021062</td>\n",
       "      <td>0.153464</td>\n",
       "      <td>-0.061993</td>\n",
       "      <td>-0.039915</td>\n",
       "      <td>0.006439</td>\n",
       "      <td>-0.062392</td>\n",
       "      <td>-0.080716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AlcoholConsump</th>\n",
       "      <td>-0.057718</td>\n",
       "      <td>0.101261</td>\n",
       "      <td>-0.036000</td>\n",
       "      <td>0.021062</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.011521</td>\n",
       "      <td>-0.003313</td>\n",
       "      <td>-0.010964</td>\n",
       "      <td>-0.023727</td>\n",
       "      <td>-0.048858</td>\n",
       "      <td>-0.037095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PhysicalActivity</th>\n",
       "      <td>-0.120440</td>\n",
       "      <td>-0.091409</td>\n",
       "      <td>0.141601</td>\n",
       "      <td>0.153464</td>\n",
       "      <td>0.011521</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.124890</td>\n",
       "      <td>-0.077884</td>\n",
       "      <td>0.005381</td>\n",
       "      <td>-0.148409</td>\n",
       "      <td>-0.253056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HighBP</th>\n",
       "      <td>0.271323</td>\n",
       "      <td>0.098372</td>\n",
       "      <td>-0.040013</td>\n",
       "      <td>-0.061993</td>\n",
       "      <td>-0.003313</td>\n",
       "      <td>-0.124890</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.298583</td>\n",
       "      <td>0.098488</td>\n",
       "      <td>0.213931</td>\n",
       "      <td>0.222913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HighChol</th>\n",
       "      <td>0.209294</td>\n",
       "      <td>0.092312</td>\n",
       "      <td>-0.040837</td>\n",
       "      <td>-0.039915</td>\n",
       "      <td>-0.010964</td>\n",
       "      <td>-0.077884</td>\n",
       "      <td>0.298583</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.086059</td>\n",
       "      <td>0.107228</td>\n",
       "      <td>0.143781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CholCheck</th>\n",
       "      <td>0.067293</td>\n",
       "      <td>-0.009377</td>\n",
       "      <td>0.024418</td>\n",
       "      <td>0.006439</td>\n",
       "      <td>-0.023727</td>\n",
       "      <td>0.005381</td>\n",
       "      <td>0.098488</td>\n",
       "      <td>0.086059</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.034688</td>\n",
       "      <td>0.040418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMI</th>\n",
       "      <td>0.225294</td>\n",
       "      <td>0.013567</td>\n",
       "      <td>-0.088325</td>\n",
       "      <td>-0.062392</td>\n",
       "      <td>-0.048858</td>\n",
       "      <td>-0.148409</td>\n",
       "      <td>0.213931</td>\n",
       "      <td>0.107228</td>\n",
       "      <td>0.034688</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.197472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiffWalk</th>\n",
       "      <td>0.223192</td>\n",
       "      <td>0.124813</td>\n",
       "      <td>-0.048510</td>\n",
       "      <td>-0.080716</td>\n",
       "      <td>-0.037095</td>\n",
       "      <td>-0.253056</td>\n",
       "      <td>0.222913</td>\n",
       "      <td>0.143781</td>\n",
       "      <td>0.040418</td>\n",
       "      <td>0.197472</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Diabetes_012     Smoke    Fruits   Veggies  AlcoholConsump  \\\n",
       "Diabetes_012          1.000000  0.064170 -0.041679 -0.058731       -0.057718   \n",
       "Smoke                 0.064170  1.000000 -0.079937 -0.031694        0.101261   \n",
       "Fruits               -0.041679 -0.079937  1.000000  0.255008       -0.036000   \n",
       "Veggies              -0.058731 -0.031694  0.255008  1.000000        0.021062   \n",
       "AlcoholConsump       -0.057718  0.101261 -0.036000  0.021062        1.000000   \n",
       "PhysicalActivity     -0.120440 -0.091409  0.141601  0.153464        0.011521   \n",
       "HighBP                0.271323  0.098372 -0.040013 -0.061993       -0.003313   \n",
       "HighChol              0.209294  0.092312 -0.040837 -0.039915       -0.010964   \n",
       "CholCheck             0.067293 -0.009377  0.024418  0.006439       -0.023727   \n",
       "BMI                   0.225294  0.013567 -0.088325 -0.062392       -0.048858   \n",
       "DiffWalk              0.223192  0.124813 -0.048510 -0.080716       -0.037095   \n",
       "\n",
       "                  PhysicalActivity    HighBP  HighChol  CholCheck       BMI  \\\n",
       "Diabetes_012             -0.120440  0.271323  0.209294   0.067293  0.225294   \n",
       "Smoke                    -0.091409  0.098372  0.092312  -0.009377  0.013567   \n",
       "Fruits                    0.141601 -0.040013 -0.040837   0.024418 -0.088325   \n",
       "Veggies                   0.153464 -0.061993 -0.039915   0.006439 -0.062392   \n",
       "AlcoholConsump            0.011521 -0.003313 -0.010964  -0.023727 -0.048858   \n",
       "PhysicalActivity          1.000000 -0.124890 -0.077884   0.005381 -0.148409   \n",
       "HighBP                   -0.124890  1.000000  0.298583   0.098488  0.213931   \n",
       "HighChol                 -0.077884  0.298583  1.000000   0.086059  0.107228   \n",
       "CholCheck                 0.005381  0.098488  0.086059   1.000000  0.034688   \n",
       "BMI                      -0.148409  0.213931  0.107228   0.034688  1.000000   \n",
       "DiffWalk                 -0.253056  0.222913  0.143781   0.040418  0.197472   \n",
       "\n",
       "                  DiffWalk  \n",
       "Diabetes_012      0.223192  \n",
       "Smoke             0.124813  \n",
       "Fruits           -0.048510  \n",
       "Veggies          -0.080716  \n",
       "AlcoholConsump   -0.037095  \n",
       "PhysicalActivity -0.253056  \n",
       "HighBP            0.222913  \n",
       "HighChol          0.143781  \n",
       "CholCheck         0.040418  \n",
       "BMI               0.197472  \n",
       "DiffWalk          1.000000  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0c57355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(187498, 5)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90b2cf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tina\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.8313 - loss: 0.4877\n",
      "Epoch 2/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8469 - loss: 0.4174\n",
      "Epoch 3/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8477 - loss: 0.4174\n",
      "Epoch 4/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8476 - loss: 0.4181\n",
      "Epoch 5/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8477 - loss: 0.4173\n",
      "Epoch 6/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8467 - loss: 0.4191\n",
      "Epoch 7/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8475 - loss: 0.4167\n",
      "Epoch 8/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8481 - loss: 0.4169\n",
      "Epoch 9/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8471 - loss: 0.4174\n",
      "Epoch 10/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8461 - loss: 0.4199\n",
      "Epoch 11/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8481 - loss: 0.4168\n",
      "Epoch 12/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8471 - loss: 0.4189\n",
      "Epoch 13/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8470 - loss: 0.4183\n",
      "Epoch 14/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8468 - loss: 0.4186\n",
      "Epoch 15/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8480 - loss: 0.4165\n",
      "Epoch 16/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8469 - loss: 0.4178\n",
      "Epoch 17/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.8476 - loss: 0.4193\n",
      "Epoch 18/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8478 - loss: 0.4177\n",
      "Epoch 19/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.8476 - loss: 0.4197\n",
      "Epoch 20/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.8469 - loss: 0.4168\n",
      "Epoch 21/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8486 - loss: 0.4167\n",
      "Epoch 22/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8467 - loss: 0.4188\n",
      "Epoch 23/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.8460 - loss: 0.4201\n",
      "Epoch 24/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8478 - loss: 0.4165\n",
      "Epoch 25/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8471 - loss: 0.4187\n",
      "Epoch 26/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8462 - loss: 0.4198\n",
      "Epoch 27/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8479 - loss: 0.4173\n",
      "Epoch 28/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8476 - loss: 0.4174\n",
      "Epoch 29/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8477 - loss: 0.4186\n",
      "Epoch 30/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8482 - loss: 0.4175\n",
      "Epoch 31/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8487 - loss: 0.4158\n",
      "Epoch 32/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8477 - loss: 0.4184\n",
      "Epoch 33/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8473 - loss: 0.4160\n",
      "Epoch 34/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8473 - loss: 0.4178\n",
      "Epoch 35/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8473 - loss: 0.4188\n",
      "Epoch 36/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8464 - loss: 0.4198\n",
      "Epoch 37/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8471 - loss: 0.4159\n",
      "Epoch 38/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8463 - loss: 0.4190\n",
      "Epoch 39/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8486 - loss: 0.4144\n",
      "Epoch 40/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8475 - loss: 0.4182\n",
      "Epoch 41/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8473 - loss: 0.4188\n",
      "Epoch 42/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8480 - loss: 0.4170\n",
      "Epoch 43/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8472 - loss: 0.4188\n",
      "Epoch 44/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - accuracy: 0.8451 - loss: 0.4217\n",
      "Epoch 45/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8484 - loss: 0.4188\n",
      "Epoch 46/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8472 - loss: 0.4182\n",
      "Epoch 47/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8481 - loss: 0.4162\n",
      "Epoch 48/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.8505 - loss: 0.4124\n",
      "Epoch 49/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8466 - loss: 0.4172\n",
      "Epoch 50/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8478 - loss: 0.4155\n",
      "Epoch 51/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8473 - loss: 0.4181\n",
      "Epoch 52/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8468 - loss: 0.4180\n",
      "Epoch 53/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8489 - loss: 0.4151\n",
      "Epoch 54/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8465 - loss: 0.4185\n",
      "Epoch 55/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8461 - loss: 0.4205\n",
      "Epoch 56/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8460 - loss: 0.4214\n",
      "Epoch 57/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8479 - loss: 0.4172\n",
      "Epoch 58/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8480 - loss: 0.4196\n",
      "Epoch 59/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8471 - loss: 0.4174\n",
      "Epoch 60/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8462 - loss: 0.4205\n",
      "Epoch 61/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8485 - loss: 0.4152\n",
      "Epoch 62/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8476 - loss: 0.4160\n",
      "Epoch 63/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8479 - loss: 0.4159\n",
      "Epoch 64/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8474 - loss: 0.4176\n",
      "Epoch 65/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8485 - loss: 0.4160\n",
      "Epoch 66/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8475 - loss: 0.4157\n",
      "Epoch 67/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8495 - loss: 0.4138\n",
      "Epoch 68/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8474 - loss: 0.4151\n",
      "Epoch 69/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8483 - loss: 0.4166\n",
      "Epoch 70/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8472 - loss: 0.4175\n",
      "Epoch 71/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8482 - loss: 0.4149\n",
      "Epoch 72/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8483 - loss: 0.4153\n",
      "Epoch 73/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8486 - loss: 0.4153\n",
      "Epoch 74/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8474 - loss: 0.4186\n",
      "Epoch 75/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8466 - loss: 0.4186\n",
      "Epoch 76/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8469 - loss: 0.4169\n",
      "Epoch 77/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8459 - loss: 0.4182\n",
      "Epoch 78/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8478 - loss: 0.4173\n",
      "Epoch 79/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8464 - loss: 0.4188\n",
      "Epoch 80/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8458 - loss: 0.4207\n",
      "Epoch 81/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.8475 - loss: 0.4165\n",
      "Epoch 82/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8477 - loss: 0.4174\n",
      "Epoch 83/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8469 - loss: 0.4194\n",
      "Epoch 84/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8481 - loss: 0.4169\n",
      "Epoch 85/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8467 - loss: 0.4192\n",
      "Epoch 86/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8484 - loss: 0.4144\n",
      "Epoch 87/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8474 - loss: 0.4162\n",
      "Epoch 88/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8475 - loss: 0.4176\n",
      "Epoch 89/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.8475 - loss: 0.4180\n",
      "Epoch 90/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8473 - loss: 0.4179\n",
      "Epoch 91/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8463 - loss: 0.4181\n",
      "Epoch 92/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8470 - loss: 0.4190\n",
      "Epoch 93/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8475 - loss: 0.4185\n",
      "Epoch 94/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8486 - loss: 0.4147\n",
      "Epoch 95/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8471 - loss: 0.4186\n",
      "Epoch 96/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8468 - loss: 0.4200\n",
      "Epoch 97/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8470 - loss: 0.4187\n",
      "Epoch 98/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8466 - loss: 0.4210\n",
      "Epoch 99/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8485 - loss: 0.4165\n",
      "Epoch 100/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8465 - loss: 0.4181\n",
      "1954/1954 - 2s - 1ms/step - accuracy: 0.8474 - loss: 0.4186\n",
      "The model of Health wellness:  Loss: 0.4186117351055145, Accuracy: 0.8474079966545105\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "num_classes = len(set(y_train))\n",
    "# Assuming X_train, X_test, y_train, and y_test are your training and testing data\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler to the training data\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the training and testing data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "encoder = OneHotEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_test_encoded = encoder.transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "# Calculate the number of classes\n",
    "num_classes = len(set(y_train))\n",
    "\n",
    "# Define the deep learning model \n",
    "nn_model = tf.keras.models.Sequential()\n",
    "nn_model.add(tf.keras.layers.Dense(units=6, activation=\"relu\", input_dim=5))\n",
    "nn_model.add(tf.keras.layers.Dense(units=5, activation=\"relu\"))\n",
    "nn_model.add(tf.keras.layers.Dense(units=num_classes, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the Sequential model with \"categorical_crossentropy\" loss\n",
    "nn_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "fit_model = nn_model.fit(X_train_scaled, y_train_encoded, epochs=100, verbose=1)\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn_model.evaluate(X_test_scaled, y_test_encoded, verbose=2)\n",
    "print(f\"The model of Health wellness:  Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3c56653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check accuracy for Behavioral columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95594f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "Behavioral = df[[\"Smoke\",\"Fruits\",\"Veggies\",\"AlcoholConsump\",\"PhysicalActivity\"]]\n",
    "#Behavioral.columns = Behavioral.columns.astype(str)\n",
    "#Behavioral_scaled = StandardScaler().fit_transform(Behavioral)\n",
    "#BV_scaled = pd.DataFrame(Behavioral_scaled)\n",
    "\n",
    "y = df[\"Diabetes_012\"].values\n",
    "X = Behavioral.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2267aa64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(187498, 5)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78, stratify=y)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bd5502f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tina\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.8363 - loss: 0.5324\n",
      "Epoch 2/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8449 - loss: 0.4772\n",
      "Epoch 3/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8426 - loss: 0.4821\n",
      "Epoch 4/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8439 - loss: 0.4769\n",
      "Epoch 5/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8433 - loss: 0.4794\n",
      "Epoch 6/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8438 - loss: 0.4780\n",
      "Epoch 7/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8424 - loss: 0.4796\n",
      "Epoch 8/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8435 - loss: 0.4781\n",
      "Epoch 9/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8447 - loss: 0.4761\n",
      "Epoch 10/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8442 - loss: 0.4775\n",
      "Epoch 11/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8427 - loss: 0.4793\n",
      "Epoch 12/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8450 - loss: 0.4756\n",
      "Epoch 13/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8451 - loss: 0.4755\n",
      "Epoch 14/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8428 - loss: 0.4794\n",
      "Epoch 15/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8438 - loss: 0.4777\n",
      "Epoch 16/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8431 - loss: 0.4798\n",
      "Epoch 17/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8433 - loss: 0.4792\n",
      "Epoch 18/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8424 - loss: 0.4804\n",
      "Epoch 19/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.8416 - loss: 0.4819\n",
      "Epoch 20/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.8442 - loss: 0.4784\n",
      "Epoch 21/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8442 - loss: 0.4762\n",
      "Epoch 22/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8436 - loss: 0.4785\n",
      "Epoch 23/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8435 - loss: 0.4788\n",
      "Epoch 24/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8428 - loss: 0.4799\n",
      "Epoch 25/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8451 - loss: 0.4753\n",
      "Epoch 26/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8435 - loss: 0.4775\n",
      "Epoch 27/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8435 - loss: 0.4785\n",
      "Epoch 28/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8435 - loss: 0.4784\n",
      "Epoch 29/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8451 - loss: 0.4749\n",
      "Epoch 30/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8430 - loss: 0.4796\n",
      "Epoch 31/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.8431 - loss: 0.4785\n",
      "Epoch 32/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.8433 - loss: 0.4779\n",
      "Epoch 33/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8446 - loss: 0.4764\n",
      "Epoch 34/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8447 - loss: 0.4756\n",
      "Epoch 35/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.8437 - loss: 0.4767\n",
      "Epoch 36/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.8452 - loss: 0.4761\n",
      "Epoch 37/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.8443 - loss: 0.4779\n",
      "Epoch 38/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8438 - loss: 0.4782\n",
      "Epoch 39/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8435 - loss: 0.4789\n",
      "Epoch 40/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8432 - loss: 0.4782\n",
      "Epoch 41/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8440 - loss: 0.4781\n",
      "Epoch 42/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8429 - loss: 0.4774\n",
      "Epoch 43/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8436 - loss: 0.4768\n",
      "Epoch 44/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8440 - loss: 0.4778\n",
      "Epoch 45/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.8449 - loss: 0.4752\n",
      "Epoch 46/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8437 - loss: 0.4773\n",
      "Epoch 47/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8433 - loss: 0.4786\n",
      "Epoch 48/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.8446 - loss: 0.4767\n",
      "Epoch 49/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8434 - loss: 0.4776\n",
      "Epoch 50/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8437 - loss: 0.4775\n",
      "Epoch 51/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.8436 - loss: 0.4792\n",
      "Epoch 52/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8442 - loss: 0.4756\n",
      "Epoch 53/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.8433 - loss: 0.4787\n",
      "Epoch 54/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8422 - loss: 0.4804\n",
      "Epoch 55/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8446 - loss: 0.4762\n",
      "Epoch 56/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8437 - loss: 0.4782\n",
      "Epoch 57/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8435 - loss: 0.4780\n",
      "Epoch 58/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8443 - loss: 0.4767\n",
      "Epoch 59/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8447 - loss: 0.4755\n",
      "Epoch 60/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8422 - loss: 0.4809\n",
      "Epoch 61/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8436 - loss: 0.4775\n",
      "Epoch 62/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8434 - loss: 0.4776\n",
      "Epoch 63/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8440 - loss: 0.4764\n",
      "Epoch 64/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8438 - loss: 0.4767\n",
      "Epoch 65/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8449 - loss: 0.4749\n",
      "Epoch 66/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8438 - loss: 0.4771\n",
      "Epoch 67/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8441 - loss: 0.4762\n",
      "Epoch 68/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8449 - loss: 0.4757\n",
      "Epoch 69/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8443 - loss: 0.4768\n",
      "Epoch 70/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8438 - loss: 0.4788\n",
      "Epoch 71/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8437 - loss: 0.4772\n",
      "Epoch 72/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8454 - loss: 0.4746\n",
      "Epoch 73/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8433 - loss: 0.4785\n",
      "Epoch 74/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8438 - loss: 0.4772\n",
      "Epoch 75/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8445 - loss: 0.4758\n",
      "Epoch 76/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8426 - loss: 0.4800\n",
      "Epoch 77/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8434 - loss: 0.4786\n",
      "Epoch 78/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8436 - loss: 0.4782\n",
      "Epoch 79/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8425 - loss: 0.4801\n",
      "Epoch 80/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8440 - loss: 0.4772\n",
      "Epoch 81/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8436 - loss: 0.4779\n",
      "Epoch 82/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8445 - loss: 0.4766\n",
      "Epoch 83/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8442 - loss: 0.4769\n",
      "Epoch 84/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8422 - loss: 0.4811\n",
      "Epoch 85/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.8431 - loss: 0.4800\n",
      "Epoch 86/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - accuracy: 0.8419 - loss: 0.4819\n",
      "Epoch 87/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8420 - loss: 0.4820\n",
      "Epoch 88/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8438 - loss: 0.4771\n",
      "Epoch 89/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8434 - loss: 0.4780\n",
      "Epoch 90/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8422 - loss: 0.4797\n",
      "Epoch 91/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8423 - loss: 0.4804\n",
      "Epoch 92/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8438 - loss: 0.4790\n",
      "Epoch 93/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8444 - loss: 0.4762\n",
      "Epoch 94/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8446 - loss: 0.4762\n",
      "Epoch 95/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8431 - loss: 0.4787\n",
      "Epoch 96/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8431 - loss: 0.4778\n",
      "Epoch 97/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8431 - loss: 0.4797\n",
      "Epoch 98/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8425 - loss: 0.4805\n",
      "Epoch 99/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8441 - loss: 0.4770\n",
      "Epoch 100/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8444 - loss: 0.4759\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "num_classes = len(set(y_train))\n",
    "# Assuming X_train, X_test, y_train, and y_test are your training and testing data\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler to the training data\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the training and testing data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "encoder = OneHotEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_test_encoded = encoder.transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "# Calculate the number of classes\n",
    "num_classes = len(set(y_train))\n",
    "\n",
    "# Define the deep learning model \n",
    "nn_model = tf.keras.models.Sequential()\n",
    "nn_model.add(tf.keras.layers.Dense(units=6, activation=\"relu\", input_dim=5))\n",
    "nn_model.add(tf.keras.layers.Dense(units=5, activation=\"relu\"))\n",
    "nn_model.add(tf.keras.layers.Dense(units=num_classes, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the Sequential model with \"categorical_crossentropy\" loss\n",
    "nn_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "fit_model = nn_model.fit(X_train_scaled, y_train_encoded, epochs=100, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4c1ee7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1954/1954 - 2s - 1ms/step - accuracy: 0.8437 - loss: 0.4773\n",
      "The model of Behavioral:  Loss: 0.47730210423469543, Accuracy: 0.8437280058860779\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn_model.evaluate(X_test_scaled, y_test_encoded, verbose=2)\n",
    "print(f\"The model of Behavioral:  Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "451f022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check accuracy for Medical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1fc45c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "Medical= df[[\"HeartDiseaseorAttack\",\"Stroke\",\"AnyHealthcare\",\"GenHlth\",\"MentHlth\",\"PhysHlth\"]]\n",
    "#Medical.columns = Medical.columns.astype(str)\n",
    "#Medical_scaled = StandardScaler().fit_transform(Medical)\n",
    "#MD_scaled = pd.DataFrame(Medical_scaled)\n",
    "\n",
    "y = df[\"Diabetes_012\"].values\n",
    "X = Medical.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e1b8b48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(187498, 6)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78, stratify=y)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d3d28522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tina\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.8268 - loss: 0.5033\n",
      "Epoch 2/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8433 - loss: 0.4392\n",
      "Epoch 3/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8433 - loss: 0.4385\n",
      "Epoch 4/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - accuracy: 0.8444 - loss: 0.4365\n",
      "Epoch 5/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8440 - loss: 0.4371\n",
      "Epoch 6/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8442 - loss: 0.4367\n",
      "Epoch 7/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8456 - loss: 0.4337\n",
      "Epoch 8/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8440 - loss: 0.4378\n",
      "Epoch 9/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8439 - loss: 0.4366\n",
      "Epoch 10/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8423 - loss: 0.4394\n",
      "Epoch 11/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8423 - loss: 0.4391\n",
      "Epoch 12/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8441 - loss: 0.4359\n",
      "Epoch 13/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8454 - loss: 0.4324\n",
      "Epoch 14/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8420 - loss: 0.4401\n",
      "Epoch 15/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8438 - loss: 0.4393\n",
      "Epoch 16/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8415 - loss: 0.4423\n",
      "Epoch 17/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8436 - loss: 0.4386\n",
      "Epoch 18/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8446 - loss: 0.4359\n",
      "Epoch 19/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8447 - loss: 0.4378\n",
      "Epoch 20/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8436 - loss: 0.4361\n",
      "Epoch 21/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8452 - loss: 0.4345\n",
      "Epoch 22/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8427 - loss: 0.4408\n",
      "Epoch 23/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8434 - loss: 0.4375\n",
      "Epoch 24/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8437 - loss: 0.4390\n",
      "Epoch 25/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8445 - loss: 0.4345\n",
      "Epoch 26/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8435 - loss: 0.4384\n",
      "Epoch 27/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8436 - loss: 0.4354\n",
      "Epoch 28/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8436 - loss: 0.4367\n",
      "Epoch 29/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8439 - loss: 0.4373\n",
      "Epoch 30/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8435 - loss: 0.4371\n",
      "Epoch 31/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8427 - loss: 0.4389\n",
      "Epoch 32/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8436 - loss: 0.4393\n",
      "Epoch 33/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8438 - loss: 0.4358\n",
      "Epoch 34/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8448 - loss: 0.4365\n",
      "Epoch 35/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8441 - loss: 0.4363\n",
      "Epoch 36/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8444 - loss: 0.4355\n",
      "Epoch 37/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8452 - loss: 0.4347\n",
      "Epoch 38/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8453 - loss: 0.4353\n",
      "Epoch 39/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8451 - loss: 0.4341\n",
      "Epoch 40/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8443 - loss: 0.4351\n",
      "Epoch 41/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8444 - loss: 0.4370\n",
      "Epoch 42/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8446 - loss: 0.4357\n",
      "Epoch 43/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8431 - loss: 0.4385\n",
      "Epoch 44/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8442 - loss: 0.4366\n",
      "Epoch 45/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8451 - loss: 0.4325\n",
      "Epoch 46/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8436 - loss: 0.4353\n",
      "Epoch 47/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8446 - loss: 0.4360\n",
      "Epoch 48/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8441 - loss: 0.4365\n",
      "Epoch 49/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8428 - loss: 0.4393\n",
      "Epoch 50/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8428 - loss: 0.4401\n",
      "Epoch 51/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8434 - loss: 0.4354\n",
      "Epoch 52/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8444 - loss: 0.4340\n",
      "Epoch 53/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8428 - loss: 0.4392\n",
      "Epoch 54/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8451 - loss: 0.4353\n",
      "Epoch 55/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8422 - loss: 0.4392\n",
      "Epoch 56/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8437 - loss: 0.4372\n",
      "Epoch 57/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8441 - loss: 0.4376\n",
      "Epoch 58/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8448 - loss: 0.4351\n",
      "Epoch 59/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8426 - loss: 0.4373\n",
      "Epoch 60/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8444 - loss: 0.4349\n",
      "Epoch 61/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8441 - loss: 0.4357\n",
      "Epoch 62/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8457 - loss: 0.4343\n",
      "Epoch 63/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8426 - loss: 0.4385\n",
      "Epoch 64/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8428 - loss: 0.4392\n",
      "Epoch 65/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8441 - loss: 0.4355\n",
      "Epoch 66/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8437 - loss: 0.4372\n",
      "Epoch 67/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8433 - loss: 0.4380\n",
      "Epoch 68/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8447 - loss: 0.4360\n",
      "Epoch 69/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - accuracy: 0.8441 - loss: 0.4359\n",
      "Epoch 70/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8459 - loss: 0.4340\n",
      "Epoch 71/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8432 - loss: 0.4376\n",
      "Epoch 72/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8429 - loss: 0.4377\n",
      "Epoch 73/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8442 - loss: 0.4362\n",
      "Epoch 74/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8424 - loss: 0.4406\n",
      "Epoch 75/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8434 - loss: 0.4388\n",
      "Epoch 76/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8429 - loss: 0.4392\n",
      "Epoch 77/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8434 - loss: 0.4378\n",
      "Epoch 78/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1ms/step - accuracy: 0.8433 - loss: 0.4378\n",
      "Epoch 79/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8442 - loss: 0.4355\n",
      "Epoch 80/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8423 - loss: 0.4381\n",
      "Epoch 81/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8435 - loss: 0.4363\n",
      "Epoch 82/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8431 - loss: 0.4379\n",
      "Epoch 83/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - accuracy: 0.8444 - loss: 0.4336\n",
      "Epoch 84/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8429 - loss: 0.4387\n",
      "Epoch 85/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8439 - loss: 0.4363\n",
      "Epoch 86/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8433 - loss: 0.4367\n",
      "Epoch 87/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8442 - loss: 0.4358\n",
      "Epoch 88/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8445 - loss: 0.4360\n",
      "Epoch 89/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8439 - loss: 0.4361\n",
      "Epoch 90/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8455 - loss: 0.4339\n",
      "Epoch 91/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8444 - loss: 0.4351\n",
      "Epoch 92/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8451 - loss: 0.4359\n",
      "Epoch 93/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8428 - loss: 0.4380\n",
      "Epoch 94/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8442 - loss: 0.4341\n",
      "Epoch 95/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8435 - loss: 0.4379\n",
      "Epoch 96/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8431 - loss: 0.4378\n",
      "Epoch 97/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8449 - loss: 0.4335\n",
      "Epoch 98/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8439 - loss: 0.4356\n",
      "Epoch 99/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8445 - loss: 0.4361\n",
      "Epoch 100/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8440 - loss: 0.4367\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "num_classes = len(set(y_train))\n",
    "# Assuming X_train, X_test, y_train, and y_test are your training and testing data\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler to the training data\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the training and testing data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "encoder = OneHotEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_test_encoded = encoder.transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "# Calculate the number of classes\n",
    "num_classes = len(set(y_train))\n",
    "\n",
    "# Define the deep learning model \n",
    "nn_model = tf.keras.models.Sequential()\n",
    "nn_model.add(tf.keras.layers.Dense(units=8, activation=\"relu\", input_dim=6))\n",
    "nn_model.add(tf.keras.layers.Dense(units=7, activation=\"relu\"))\n",
    "nn_model.add(tf.keras.layers.Dense(units=num_classes, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the Sequential model with \"categorical_crossentropy\" loss\n",
    "nn_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "fit_model = nn_model.fit(X_train_scaled, y_train_encoded, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "54711482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1954/1954 - 2s - 1ms/step - accuracy: 0.8438 - loss: 0.4350\n",
      "The model of Medical:  Loss: 0.43500185012817383, Accuracy: 0.8438079953193665\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn_model.evaluate(X_test_scaled, y_test_encoded, verbose=2)\n",
    "print(f\"The model of Medical:  Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1eed7a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check accuracy for Bio columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bf85f627",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bio = df[[\"Sex\",\"Age\",\"Education\",\"NoDocbcCost\"]]\n",
    "#Bio.columns = Bio.columns.astype(str)\n",
    "#Bio_scaled = StandardScaler().fit_transform(Bio)\n",
    "#Bio_scaled = pd.DataFrame(Bio_scaled)\n",
    "\n",
    "y = df[\"Diabetes_012\"].values\n",
    "X = Bio.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "37c4e8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(187498, 4)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78, stratify=y)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "52c5affc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187498"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "849ceabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tina\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_13\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_13\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_39 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │            \u001b[38;5;34m40\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_40 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │            \u001b[38;5;34m63\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m24\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">127</span> (508.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m127\u001b[0m (508.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">127</span> (508.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m127\u001b[0m (508.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "num_classes = len(set(y_train))\n",
    "# Assuming X_train, X_test, y_train, and y_test are your training and testing data\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler to the training data\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the training and testing data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "encoder = OneHotEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_test_encoded = encoder.transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "\n",
    "# Define the deep learning model \n",
    "nn_model = tf.keras.models.Sequential()\n",
    "nn_model.add(tf.keras.layers.Dense(units=8, activation=\"relu\", input_dim=4))\n",
    "nn_model.add(tf.keras.layers.Dense(units=7, activation=\"relu\"))\n",
    "nn_model.add(tf.keras.layers.Dense(units=num_classes, activation=\"sigmoid\"))\n",
    "nn_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9aa73e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8386 - loss: 0.4985\n",
      "Epoch 2/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8452 - loss: 0.4572\n",
      "Epoch 3/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8428 - loss: 0.4603\n",
      "Epoch 4/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8451 - loss: 0.4558\n",
      "Epoch 5/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8425 - loss: 0.4605\n",
      "Epoch 6/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8432 - loss: 0.4600\n",
      "Epoch 7/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8456 - loss: 0.4557\n",
      "Epoch 8/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8429 - loss: 0.4593\n",
      "Epoch 9/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8445 - loss: 0.4567\n",
      "Epoch 10/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8438 - loss: 0.4576\n",
      "Epoch 11/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8422 - loss: 0.4609\n",
      "Epoch 12/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8430 - loss: 0.4593\n",
      "Epoch 13/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8440 - loss: 0.4577\n",
      "Epoch 14/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8420 - loss: 0.4614\n",
      "Epoch 15/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8437 - loss: 0.4582\n",
      "Epoch 16/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8427 - loss: 0.4599\n",
      "Epoch 17/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8453 - loss: 0.4553\n",
      "Epoch 18/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8425 - loss: 0.4597\n",
      "Epoch 19/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8442 - loss: 0.4570\n",
      "Epoch 20/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8437 - loss: 0.4580\n",
      "Epoch 21/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8450 - loss: 0.4558\n",
      "Epoch 22/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8435 - loss: 0.4586\n",
      "Epoch 23/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8456 - loss: 0.4544\n",
      "Epoch 24/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8426 - loss: 0.4591\n",
      "Epoch 25/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8464 - loss: 0.4539\n",
      "Epoch 26/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8435 - loss: 0.4581\n",
      "Epoch 27/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8434 - loss: 0.4586\n",
      "Epoch 28/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8450 - loss: 0.4560\n",
      "Epoch 29/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8428 - loss: 0.4605\n",
      "Epoch 30/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8445 - loss: 0.4558\n",
      "Epoch 31/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8437 - loss: 0.4589\n",
      "Epoch 32/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8433 - loss: 0.4577\n",
      "Epoch 33/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8427 - loss: 0.4607\n",
      "Epoch 34/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8438 - loss: 0.4575\n",
      "Epoch 35/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8448 - loss: 0.4563\n",
      "Epoch 36/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8431 - loss: 0.4599\n",
      "Epoch 37/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8430 - loss: 0.4600\n",
      "Epoch 38/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8430 - loss: 0.4584\n",
      "Epoch 39/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8435 - loss: 0.4561\n",
      "Epoch 40/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8429 - loss: 0.4597\n",
      "Epoch 41/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8428 - loss: 0.4585\n",
      "Epoch 42/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8440 - loss: 0.4575\n",
      "Epoch 43/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8450 - loss: 0.4560\n",
      "Epoch 44/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8433 - loss: 0.4590\n",
      "Epoch 45/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8415 - loss: 0.4620\n",
      "Epoch 46/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8437 - loss: 0.4565\n",
      "Epoch 47/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8440 - loss: 0.4567\n",
      "Epoch 48/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8444 - loss: 0.4557\n",
      "Epoch 49/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8443 - loss: 0.4565\n",
      "Epoch 50/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8441 - loss: 0.4567\n",
      "Epoch 51/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8442 - loss: 0.4576\n",
      "Epoch 52/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8432 - loss: 0.4592\n",
      "Epoch 53/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8449 - loss: 0.4545\n",
      "Epoch 54/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8448 - loss: 0.4558\n",
      "Epoch 55/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8444 - loss: 0.4567\n",
      "Epoch 56/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8437 - loss: 0.4585\n",
      "Epoch 57/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8429 - loss: 0.4591\n",
      "Epoch 58/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8434 - loss: 0.4566\n",
      "Epoch 59/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.8447 - loss: 0.4565\n",
      "Epoch 60/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8438 - loss: 0.4568\n",
      "Epoch 61/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8450 - loss: 0.4563\n",
      "Epoch 62/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8442 - loss: 0.4582\n",
      "Epoch 63/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8444 - loss: 0.4579\n",
      "Epoch 64/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8420 - loss: 0.4604\n",
      "Epoch 65/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8422 - loss: 0.4617\n",
      "Epoch 66/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8435 - loss: 0.4589\n",
      "Epoch 67/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8431 - loss: 0.4590\n",
      "Epoch 68/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8433 - loss: 0.4588\n",
      "Epoch 69/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8419 - loss: 0.4609\n",
      "Epoch 70/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8433 - loss: 0.4586\n",
      "Epoch 71/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8437 - loss: 0.4576\n",
      "Epoch 72/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8435 - loss: 0.4572\n",
      "Epoch 73/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8449 - loss: 0.4561\n",
      "Epoch 74/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8449 - loss: 0.4554\n",
      "Epoch 75/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8435 - loss: 0.4573\n",
      "Epoch 76/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8441 - loss: 0.4575\n",
      "Epoch 77/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8434 - loss: 0.4573\n",
      "Epoch 78/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8438 - loss: 0.4569\n",
      "Epoch 79/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - accuracy: 0.8453 - loss: 0.4563\n",
      "Epoch 80/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8447 - loss: 0.4560\n",
      "Epoch 81/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8436 - loss: 0.4564\n",
      "Epoch 82/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8455 - loss: 0.4533\n",
      "Epoch 83/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8441 - loss: 0.4552\n",
      "Epoch 84/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8441 - loss: 0.4571\n",
      "Epoch 85/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8434 - loss: 0.4588\n",
      "Epoch 86/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8438 - loss: 0.4571\n",
      "Epoch 87/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8433 - loss: 0.4585\n",
      "Epoch 88/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8448 - loss: 0.4564\n",
      "Epoch 89/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8420 - loss: 0.4610\n",
      "Epoch 90/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8422 - loss: 0.4606\n",
      "Epoch 91/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8426 - loss: 0.4606\n",
      "Epoch 92/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8434 - loss: 0.4586\n",
      "Epoch 93/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8451 - loss: 0.4548\n",
      "Epoch 94/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8431 - loss: 0.4600\n",
      "Epoch 95/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8428 - loss: 0.4598\n",
      "Epoch 96/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8425 - loss: 0.4595\n",
      "Epoch 97/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8430 - loss: 0.4588\n",
      "Epoch 98/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.8436 - loss: 0.4574\n",
      "Epoch 99/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8428 - loss: 0.4599\n",
      "Epoch 100/100\n",
      "\u001b[1m5860/5860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.8440 - loss: 0.4574\n"
     ]
    }
   ],
   "source": [
    "# Compile the Sequential model with \"categorical_crossentropy\" loss\n",
    "nn_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "fit_model = nn_model.fit(X_train_scaled, y_train_encoded, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "887174d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1954/1954 - 2s - 1ms/step - accuracy: 0.8437 - loss: 0.4579\n",
      "The model of Medical:  Loss: 0.4578651785850525, Accuracy: 0.8437280058860779\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn_model.evaluate(X_test_scaled, y_test_encoded, verbose=2)\n",
    "print(f\"The model of Medical:  Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
